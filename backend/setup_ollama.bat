@echo off
REM RAPID-100 Ollama Setup Script for Windows
REM Automates setup of Ollama and model creation

echo.
echo â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
echo â•‘        RAPID-100 Emergency Triage - Ollama Setup          â•‘
echo â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo.

REM Check if Ollama is installed
where ollama >nul 2>nul
if %ERRORLEVEL% NEQ 0 (
    echo âŒ Ollama is not installed!
    echo.
    echo ğŸ“¥ Please install Ollama first:
    echo    1. Visit: https://ollama.ai
    echo    2. Download and install for Windows
    echo    3. Run: ollama serve (in another terminal)
    echo.
    pause
    exit /b 1
)

echo âœ… Ollama is installed
echo.

REM Check if Ollama server is running
echo ğŸ” Checking if Ollama server is running...
ollama list >nul 2>nul
if %ERRORLEVEL% NEQ 0 (
    echo âš ï¸  Ollama server doesn't appear to be running
    echo.
    echo Start Ollama in another terminal:
    echo    ollama serve
    echo.
    pause
)

echo âœ… Ollama server is accessible
echo.

REM Navigate to backend directory
echo ğŸ“ Navigating to backend directory...
cd /d "%~dp0"
echo ğŸ“ Current directory: %cd%
echo.

REM Check if Modelfile exists
if not exist "Modelfile" (
    echo âŒ Modelfile not found!
    pause
    exit /b 1
)

echo âœ… Modelfile found
echo.

REM Create the rapid-triage model
echo ğŸ¤– Creating RAPID-100 emergency triage model...
ollama create rapid-triage -f Modelfile

if %ERRORLEVEL% NEQ 0 (
    echo âŒ Failed to create model
    pause
    exit /b 1
)

echo âœ… Model created successfully!
echo.

REM Verify model
echo ğŸ“‹ Verifying model...
ollama list | findstr "rapid-triage"
echo.

REM Install Python dependencies
echo ğŸ“¦ Installing Python dependencies...
if exist "requirements.txt" (
    pip install -r requirements.txt
    echo âœ… Dependencies installed
) else (
    echo âš ï¸  requirements.txt not found
)

echo.
echo â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
echo â•‘           âœ… Setup Complete!                              â•‘
echo â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo.
echo ğŸš€ Next Steps:
echo.
echo 1. Ensure Ollama is running:
echo    ollama serve
echo.
echo 2. Start the backend:
echo    python main.py
echo.
echo 3. Run tests:
echo    python execute.py
echo.
echo ğŸ“– For more details, see: OLLAMA_INTEGRATION.md
echo.
pause
